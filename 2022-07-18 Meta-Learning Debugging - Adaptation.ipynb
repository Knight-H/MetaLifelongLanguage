{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, gc, os, pickle\n",
    "\n",
    "import datasets.utils\n",
    "import models.utils\n",
    "from models.cls_oml_ori_v2 import OML\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import higher\n",
    "import torch\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_order_mapping = {\n",
    "    1: [2, 0, 3, 1, 4],\n",
    "    2: [3, 4, 0, 1, 2],\n",
    "    3: [2, 4, 1, 3, 0],\n",
    "    4: [0, 2, 1, 4, 3]\n",
    "}\n",
    "n_classes = 33\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "model_path = \"saved_models/OML-2022-04-16_18-19-52.158289.pt\"\n",
    "# model_path = \"saved_models/OML-2022-04-17_14-40-54.970677.pt\"\n",
    "# model_path = \"saved_models/OML-2022-04-17_18-46-49.700106.pt\"\n",
    "# model_path = \"saved_models/OML-2022-04-18_06-16-55.619110.pt\"\n",
    "model_path = \"/data/model_runs/original_oml/OML-order1-id4-2022-07-18_17-53-13.518612.pt\"\n",
    "model_path = \"/data/model_runs/original_oml/OML-order1-id3-2022-07-18_17-22-11.802295.pt\"\n",
    "model_path = \"/data/model_runs/original_oml/OML-order2-id2-2022-07-18_19-54-42.201054.pt\"\n",
    "model_path = \"/data/model_runs/original_oml/OML-order2-id4-2022-07-18_20-56-32.905358.pt\"\n",
    "model_path = \"/data/model_runs/original_oml/OML-order1-id1-2022-07-18_16-26-09.710679.pt\"\n",
    "\n",
    "\n",
    "model_path = \"/data/model_runs/original_oml/aOML-order1-inlr002-2022-07-31/OML-order1-id4-2022-07-31_14-53-46.456804.pt\"\n",
    "\n",
    "\n",
    "\n",
    "# adaptation_updates = 5\n",
    "# adaptation_offset = 0\n",
    "adaptation_updates_list = [2,5,10]\n",
    "adaptation_offset_list = [0, 5, 100, 110, 120, 130, 140, 150, 200]\n",
    "\n",
    "data_id_support =3\n",
    "data_id_query = 1\n",
    "\n",
    "use_db_cache = True\n",
    "cache_dir = 'tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"order\": 1,\n",
    "    \"n_epochs\": 1,\n",
    "    \"lr\": 3e-5,\n",
    "    \"inner_lr\": 0.001*2,\n",
    "    \"meta_lr\": 3e-5,\n",
    "    \"model\": \"bert\",\n",
    "    \"learner\": \"oml\",\n",
    "    \"mini_batch_size\": 16,\n",
    "    \"updates\": 5,\n",
    "    \"write_prob\": 1.0,\n",
    "    \"max_length\": 448,\n",
    "    \"seed\": 42,\n",
    "    \"replay_rate\": 0.01,\n",
    "    \"replay_every\": 9600\n",
    "}\n",
    "updates = args[\"updates\"]\n",
    "mini_batch_size = args[\"mini_batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args[\"seed\"])\n",
    "random.seed(args[\"seed\"])\n",
    "np.random.seed(args[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Support Set DBPediaDataset\n",
      "Loaded Query Set AmazonDataset\n"
     ]
    }
   ],
   "source": [
    "if use_db_cache:\n",
    "    test_dataset_support_file = os.path.join(cache_dir, f\"{data_id_support}.cache\")\n",
    "    if os.path.exists(test_dataset_support_file):\n",
    "        with open(test_dataset_support_file, 'rb') as f:\n",
    "            test_dataset_support = pickle.load(f)\n",
    "    else:\n",
    "        test_dataset_support = datasets.utils.get_dataset_test(\"\", data_id_support)\n",
    "        test_dataset_support = datasets.utils.offset_labels(test_dataset_support)\n",
    "        pickle.dump(test_dataset_support, open( test_dataset_support_file, \"wb\" ), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"Pickle saved at {test_dataset_support_file}\")\n",
    "    print('Loaded Support Set {}'.format(test_dataset_support.__class__.__name__))\n",
    "    \n",
    "    if data_id_support == data_id_query:\n",
    "        test_dataset_query = test_dataset_support\n",
    "    else:\n",
    "        test_dataset_query_file = os.path.join(cache_dir, f\"{data_id_query}.cache\")\n",
    "        if os.path.exists(test_dataset_query_file):\n",
    "            with open(test_dataset_query_file, 'rb') as f:\n",
    "                test_dataset_query = pickle.load(f)\n",
    "        else:\n",
    "            test_dataset_query = datasets.utils.get_dataset_test(\"\", data_id_query)\n",
    "            test_dataset_query = datasets.utils.offset_labels(test_dataset_query)\n",
    "            pickle.dump(test_dataset_query, open( test_dataset_query_file, \"wb\" ), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(f\"Pickle saved at {test_dataset_query_file}\")\n",
    "    print('Loaded Query Set {}'.format(test_dataset_query.__class__.__name__))\n",
    "    \n",
    "else:\n",
    "    test_dataset_support = datasets.utils.get_dataset_test(\"\", data_id_support)\n",
    "    test_dataset_support = datasets.utils.offset_labels(test_dataset_support)\n",
    "    print('Loaded Support Set {}'.format(test_dataset_support.__class__.__name__))\n",
    "\n",
    "    if data_id_support == data_id_query:\n",
    "        test_dataset_query = test_dataset_support\n",
    "    else:\n",
    "        test_dataset_query = datasets.utils.get_dataset_test(\"\", data_id_query)\n",
    "        test_dataset_query = datasets.utils.offset_labels(test_dataset_query)\n",
    "    print('Loaded Query Set {}'.format(test_dataset_query.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader_support = data.DataLoader(test_dataset_support, batch_size=mini_batch_size, shuffle=False,\n",
    "                                  collate_fn=datasets.utils.batch_encode)\n",
    "test_dataloader_support_iter = iter(test_dataloader_support)\n",
    "\n",
    "test_dataloader_query = data.DataLoader(test_dataset_query, batch_size=mini_batch_size, shuffle=False,\n",
    "                                  collate_fn=datasets.utils.batch_encode)\n",
    "test_dataloader_query_iter = iter(test_dataloader_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 18, 17, 15, 17, 11, 13, 13, 9, 20, 14, 15, 9, 12, 9, 22]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, labels = next(iter(test_dataloader_support))\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 10:17:04,506 - transformers.tokenization_utils_base - INFO - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-08-01 10:17:05,739 - transformers.configuration_utils - INFO - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "2022-08-01 10:17:05,743 - transformers.configuration_utils - INFO - Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2022-08-01 10:17:05,792 - transformers.modeling_utils - INFO - loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "2022-08-01 10:17:08,523 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "2022-08-01 10:17:08,530 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "2022-08-01 10:17:12,139 - OML-Log - INFO - Loaded TransformerRLN as RLN\n",
      "2022-08-01 10:17:12,141 - OML-Log - INFO - Loaded LinearPLN as PLN\n"
     ]
    }
   ],
   "source": [
    "learner = OML(device=device, n_classes=n_classes, **args)\n",
    "learner.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Adaptation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently in adaptation offset of 0\n",
      "1.1376738548278809\n",
      "0.5625 0.6833333333333333 0.6 0.6186868686868687\n",
      "Currently in adaptation offset of 5\n",
      "1.1727596521377563\n",
      "0.5 0.6033333333333333 0.5166666666666666 0.490952380952381\n",
      "Currently in adaptation offset of 100\n",
      "1.5657199621200562\n",
      "0.625 0.7333333333333332 0.6666666666666666 0.6285714285714287\n",
      "Currently in adaptation offset of 110\n",
      "0.7064863443374634\n",
      "0.75 0.7333333333333333 0.65 0.64\n",
      "Currently in adaptation offset of 120\n",
      "1.4243059158325195\n",
      "0.5625 0.9166666666666666 0.6488095238095237 0.7333333333333333\n",
      "Currently in adaptation offset of 130\n",
      "1.1412097215652466\n",
      "0.625 0.725 0.65 0.6005128205128205\n",
      "Currently in adaptation offset of 140\n",
      "0.9943897128105164\n",
      "0.5 0.5733333333333335 0.5476190476190477 0.5421212121212121\n",
      "Currently in adaptation offset of 150\n",
      "1.5085012912750244\n",
      "0.625 0.8452380952380952 0.5833333333333334 0.6506410256410255\n",
      "Currently in adaptation offset of 200\n",
      "0.9491214156150818\n",
      "0.6875 0.77 0.7166666666666667 0.718095238095238\n",
      "\n",
      "COPY PASTA\n",
      "0.5625\n",
      "0.5\n",
      "0.625\n",
      "0.75\n",
      "0.5625\n",
      "0.625\n",
      "0.5\n",
      "0.625\n",
      "0.6875\n"
     ]
    }
   ],
   "source": [
    "offset_arr = []\n",
    "for adaptation_offset in adaptation_offset_list:\n",
    "    print(f\"Currently in adaptation offset of {adaptation_offset}\")\n",
    "    \n",
    "    test_dataloader_support_iter = iter(test_dataloader_support)\n",
    "    test_dataloader_query_iter = iter(test_dataloader_query)\n",
    "\n",
    "    learner.rln.eval()\n",
    "    learner.pln.eval()\n",
    "\n",
    "    # For Offsetting dataset\n",
    "    if adaptation_offset > 0:\n",
    "        for _ in range(adaptation_offset):\n",
    "            text, labels = next(test_dataloader_support_iter)\n",
    "            text, labels = next(test_dataloader_query_iter)\n",
    "\n",
    "    with higher.innerloop_ctx(learner.pln, learner.inner_optimizer,\n",
    "                              copy_initial_weights=False,\n",
    "                              track_higher_grads=False) as (fpln, diffopt):\n",
    "        # Inner Loop\n",
    "        support_set = []\n",
    "        for _ in range(updates):\n",
    "            text, labels = next(test_dataloader_support_iter)\n",
    "            text, labels = next(test_dataloader_query_iter)\n",
    "            support_set.append((text, labels))\n",
    "        # Outer loop\n",
    "        query_set = []\n",
    "        text, labels = next(test_dataloader_query_iter)\n",
    "        query_set.append((text, labels))\n",
    "        for text, labels in query_set:\n",
    "            labels = torch.tensor(labels).to(device)\n",
    "            input_dict = learner.rln.encode_text(text)\n",
    "            _repr = learner.rln(input_dict)\n",
    "            output = fpln(_repr)\n",
    "            loss = learner.loss_fn(output, labels)\n",
    "            print(loss.item())\n",
    "            pred = models.utils.make_prediction(output.detach())\n",
    "            acc, prec, rec, f1 = models.utils.calculate_metrics(pred.tolist(), labels.tolist())\n",
    "\n",
    "            print(acc, prec, rec, f1)\n",
    "            offset_arr.append(acc)\n",
    "print()\n",
    "print(\"COPY PASTA\")\n",
    "for row in offset_arr:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del learner\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache() \n",
    "# learner = OML(device=device, n_classes=n_classes, **args)\n",
    "# learner.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Adaptation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tCurrently in adaptation offset of 0\n",
      "Currently in adaptation updates of 2\n",
      "0.5625 0.6833333333333333 0.6 0.6186868686868687\n",
      "Currently in adaptation updates of 5\n",
      "0.5625 0.6833333333333333 0.6 0.6186868686868687\n",
      "Currently in adaptation updates of 10\n",
      "0.5625 0.6833333333333333 0.6 0.6186868686868687\n",
      "\n",
      "\tCurrently in adaptation offset of 5\n",
      "Currently in adaptation updates of 2\n",
      "0.5 0.6033333333333333 0.5166666666666666 0.490952380952381\n",
      "Currently in adaptation updates of 5\n",
      "0.5 0.6033333333333333 0.5166666666666666 0.490952380952381\n",
      "Currently in adaptation updates of 10\n",
      "0.5 0.6033333333333333 0.5166666666666666 0.490952380952381\n",
      "\n",
      "\tCurrently in adaptation offset of 100\n",
      "Currently in adaptation updates of 2\n",
      "0.625 0.7333333333333332 0.6666666666666666 0.6285714285714287\n",
      "Currently in adaptation updates of 5\n",
      "0.625 0.7333333333333332 0.6666666666666666 0.6285714285714287\n",
      "Currently in adaptation updates of 10\n",
      "0.625 0.7333333333333332 0.6666666666666666 0.6285714285714287\n",
      "\n",
      "\tCurrently in adaptation offset of 110\n",
      "Currently in adaptation updates of 2\n",
      "0.75 0.7333333333333333 0.65 0.64\n",
      "Currently in adaptation updates of 5\n",
      "0.75 0.7333333333333333 0.65 0.64\n",
      "Currently in adaptation updates of 10\n",
      "0.75 0.7333333333333333 0.65 0.64\n",
      "\n",
      "\tCurrently in adaptation offset of 120\n",
      "Currently in adaptation updates of 2\n",
      "0.5625 0.9166666666666666 0.6488095238095237 0.7333333333333333\n",
      "Currently in adaptation updates of 5\n",
      "0.5625 0.9166666666666666 0.6488095238095237 0.7333333333333333\n",
      "Currently in adaptation updates of 10\n",
      "0.5625 0.9166666666666666 0.6488095238095237 0.7333333333333333\n",
      "\n",
      "\tCurrently in adaptation offset of 130\n",
      "Currently in adaptation updates of 2\n",
      "0.625 0.725 0.65 0.6005128205128205\n",
      "Currently in adaptation updates of 5\n",
      "0.625 0.725 0.65 0.6005128205128205\n",
      "Currently in adaptation updates of 10\n",
      "0.625 0.725 0.65 0.6005128205128205\n",
      "\n",
      "\tCurrently in adaptation offset of 140\n",
      "Currently in adaptation updates of 2\n",
      "0.5 0.5733333333333335 0.5476190476190477 0.5421212121212121\n",
      "Currently in adaptation updates of 5\n",
      "0.5 0.5733333333333335 0.5476190476190477 0.5421212121212121\n",
      "Currently in adaptation updates of 10\n",
      "0.5 0.5733333333333335 0.5476190476190477 0.5421212121212121\n",
      "\n",
      "\tCurrently in adaptation offset of 150\n",
      "Currently in adaptation updates of 2\n",
      "0.625 0.8452380952380952 0.5833333333333334 0.6506410256410255\n",
      "Currently in adaptation updates of 5\n",
      "0.625 0.8452380952380952 0.5833333333333334 0.6506410256410255\n",
      "Currently in adaptation updates of 10\n",
      "0.625 0.8452380952380952 0.5833333333333334 0.6506410256410255\n",
      "\n",
      "\tCurrently in adaptation offset of 200\n",
      "Currently in adaptation updates of 2\n",
      "0.6875 0.77 0.7166666666666667 0.718095238095238\n",
      "Currently in adaptation updates of 5\n",
      "0.6875 0.77 0.7166666666666667 0.718095238095238\n",
      "Currently in adaptation updates of 10\n",
      "0.6875 0.77 0.7166666666666667 0.718095238095238\n",
      "COPY PASTA\n",
      "0.5625\t0.5625\t0.5625\n",
      "0.5\t0.5\t0.5\n",
      "0.625\t0.625\t0.625\n",
      "0.75\t0.75\t0.75\n",
      "0.5625\t0.5625\t0.5625\n",
      "0.625\t0.625\t0.625\n",
      "0.5\t0.5\t0.5\n",
      "0.625\t0.625\t0.625\n",
      "0.6875\t0.6875\t0.6875\n"
     ]
    }
   ],
   "source": [
    "all_arr = []\n",
    "\n",
    "for adaptation_offset in adaptation_offset_list:\n",
    "    print()\n",
    "    print(f\"\\tCurrently in adaptation offset of {adaptation_offset}\")\n",
    "    \n",
    "    offset_arr = []\n",
    "    for adaptation_updates in adaptation_updates_list:\n",
    "        print(f\"Currently in adaptation updates of {adaptation_updates}\")\n",
    "        learner.load_model(model_path)\n",
    "\n",
    "        test_dataloader_support_iter = iter(test_dataloader_support)\n",
    "        test_dataloader_query_iter = iter(test_dataloader_query)\n",
    "\n",
    "        learner.rln.eval()\n",
    "        learner.pln.train()\n",
    "\n",
    "        # For Offsetting dataset\n",
    "        if adaptation_offset > 0:\n",
    "            for _ in range(adaptation_offset):\n",
    "                text, labels = next(test_dataloader_support_iter)\n",
    "                text, labels = next(test_dataloader_query_iter)\n",
    "\n",
    "        with higher.innerloop_ctx(learner.pln, learner.inner_optimizer,\n",
    "                                  copy_initial_weights=False,\n",
    "                                  track_higher_grads=False) as (fpln, diffopt):\n",
    "            # Inner Loop\n",
    "            task_predictions, task_labels = [], []\n",
    "            support_set = []\n",
    "            query_set = []\n",
    "\n",
    "            if adaptation_updates == 2:\n",
    "                for ii in range(updates):\n",
    "                    text, labels = next(test_dataloader_support_iter)\n",
    "                    _, _ = next(test_dataloader_query_iter)\n",
    "                    if ii < 2: \n",
    "                        support_set.append((text, labels))\n",
    "                text, labels = next(test_dataloader_query_iter)\n",
    "                query_set.append((text, labels))\n",
    "            elif adaptation_updates == 5:\n",
    "                for _ in range(updates):\n",
    "                    text, labels = next(test_dataloader_support_iter)\n",
    "                    _, _ = next(test_dataloader_query_iter)\n",
    "                    support_set.append((text, labels))\n",
    "                text, labels = next(test_dataloader_query_iter)\n",
    "                query_set.append((text, labels))\n",
    "            elif adaptation_updates == 10:\n",
    "                for _ in range(updates):\n",
    "                    text, labels = next(test_dataloader_support_iter)\n",
    "                    _, _ = next(test_dataloader_query_iter)\n",
    "                    support_set.append((text, labels))\n",
    "                text, labels = next(test_dataloader_query_iter)\n",
    "                query_set.append((text, labels))\n",
    "                for _ in range(updates):\n",
    "                    text, labels = next(test_dataloader_support_iter)\n",
    "                    support_set.append((text, labels))\n",
    "            else:\n",
    "                raise Exception(\"ERROR adaptation_updates\")\n",
    "\n",
    "            for text, labels in support_set:\n",
    "                labels = torch.tensor(labels).to(device)\n",
    "                input_dict = learner.rln.encode_text(text)\n",
    "                _repr = learner.rln(input_dict)\n",
    "                output = fpln(_repr)\n",
    "                loss = learner.loss_fn(output, labels)\n",
    "#                 print(loss.item())\n",
    "                diffopt.step(loss)\n",
    "                pred = models.utils.make_prediction(output.detach())\n",
    "                task_predictions.extend(pred.tolist())\n",
    "                task_labels.extend(labels.tolist())\n",
    "\n",
    "            acc, prec, rec, f1 = models.utils.calculate_metrics(task_predictions, task_labels)\n",
    "#             print(acc, prec, rec, f1)\n",
    "\n",
    "            # Outer loop\n",
    "            for text, labels in query_set:\n",
    "                labels = torch.tensor(labels).to(device)\n",
    "                input_dict = learner.rln.encode_text(text)\n",
    "                _repr = learner.rln(input_dict)\n",
    "                output = fpln(_repr)\n",
    "                loss = learner.loss_fn(output, labels)\n",
    "#                 print(loss.item())\n",
    "                pred = models.utils.make_prediction(output.detach())\n",
    "                acc, prec, rec, f1 = models.utils.calculate_metrics(pred.tolist(), labels.tolist())\n",
    "\n",
    "                print(acc, prec, rec, f1)\n",
    "                offset_arr.append(acc)\n",
    "    all_arr.append(offset_arr)\n",
    "\n",
    "print(\"COPY PASTA\")\n",
    "for row in all_arr:\n",
    "    print(\"\\t\".join(str(i) for i in row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filthy divine kicks ass. ok, i like it! they have a great diversity of sound and they really kick ass! the guitar work borders on brilliant and their lead singer is total eye candy! i'm anxiously awaiting their next release - c'mon guys, when's it coming out?\n",
      "Predicted\t4\n",
      "Actual\t4\n",
      "\n",
      "very good cd, almost as good as soundbombing two. i've been listening to the cd for 2 days now and i really like most of the songs off of it. some aren't my style but they're still good. this cd's almost as good as the soundbombing 2 cd. definately worth the money.\n",
      "Predicted\t3\n",
      "Actual\t3\n",
      "\n",
      "poorly mixed. the main problem i have with this cd is that it seems poorly mixed. there is no sonic depth to it - all the sounds are flat and hard to distinguish from each other.\n",
      "Predicted\t1\n",
      "Actual\t1\n",
      "\n",
      "adopting a chinese daughter prior to the ccca. baby in a box is the story of one missionary family's struggle to bring together an abandoned chinese daughter and an american family. the union takes place in china prior to the establishment of the ccca and centralization of china's international adoption procedures. while a story of adoption, revelations regarding chinese culture are telling. together, this family's experience will be interesting to those who have adopted and/or are planning to adopt, as well as those with interest in christian missionaries' struggle in china.\n",
      "Predicted\t32\n",
      "Actual\t3\n",
      "\n",
      "looks good but. has a poor sound quality, the speakers are cheap and the reception of fm stations leaves a lot to be desired. very easy to use and prgram. overall, i think it is a bit pricey for what it delivers, like most of philips products has a very nice design but the quality is a bit on the lower side.\n",
      "Predicted\t2\n",
      "Actual\t2\n",
      "\n",
      "great lullaby. this is a great cd for newborns and adults alike. u2 translates well to lullaby music, and i am highly recommend any of these cd's from rockabye baby!\n",
      "Predicted\t4\n",
      "Actual\t4\n",
      "\n",
      "the buddenbrooks on dvd. when i was a teenager, growing up in hamburg, germany (which isabout 1-1/2 hours away from luebeck, the setting for the buddenbrook history) i read the thomas mann book, which my parents had in our library.of course, now that i'm 81, i didn't remember too much of thebook, but the video brought much of it back to me.i liked it, but i believe that not too many people, not acquainted with that region of northern germany, and thehistory of the cities belonging to the hanseatic league, which include hamburg, luebeck, bremen and a few others, would bethat interested in this family's story.still, it is an epic story of a bygone era, and i enjoyed seeing it.edith e. finke\n",
      "Predicted\t4\n",
      "Actual\t3\n",
      "\n",
      "good screen. the screen itself works ok. but it looks like a cheap product. everything is made of plastic. the rod on the bottom is wrapped with the bottom end of screen fabric and stapled together. and the plastic smell was very strong. it had a few dark scratchs on it and could not be easily removed. we returned it.\n",
      "Predicted\t2\n",
      "Actual\t1\n",
      "\n",
      "we love it. can't believe how long i cooked before i discovered grill pans. this pan is great - lightweight but really well made. i've cooked steaks, chicken, seafood and they have all turned out super. still can't beat grilling outdoors, but in the winter it is a great alternative. cleanup is very easy, takes a little effort get into the grooves. so far the cooking surface seems to be wearing very well. excellent value, would highly recommend.\n",
      "Predicted\t4\n",
      "Actual\t4\n",
      "\n",
      "it was a sort of funny book, but very shallow. i did not like this book very much because the plot is too simple. there is an interesting premise, but the jokes are pretty corny!\n",
      "Predicted\t1\n",
      "Actual\t2\n",
      "\n",
      "great no frills lap top bag. sad to see this bag was discontinued. it was a great item - the perfect thing for my 17 inch powerbook. there is a new version, i believe but it has slightly more feminine styling, at least to my eyes. only thing missing from this guy was an internal divider in the main pocket - a place to let you store a legal pad and a few papers without it rubbing on the computer. ebags makes quality stuff. i wouldn't hesitate to buy one of their in-house branded products again, given my good experience with this bag.\n",
      "Predicted\t3\n",
      "Actual\t3\n",
      "\n",
      "becoming madam mao. if you like politics and government theories, this may be your book. writing not the best. moves unevenly between narrator and main character. timelines difficult to follow.\n",
      "Predicted\t1\n",
      "Actual\t2\n",
      "\n",
      "dull and not funny. i have always been a fan of romantic comedies, and i've even been able to enjoy formulaic ones such as the wedding planner and it could happen to you, but this movie is just too awful. the acting is atrocious and cher is just way too old for the part (is she supposed to look fifty?). personally i love nicholas cage but there is nothing appealing about his character in this movie and the thick black hair all over his body certainly doesn't help.none of this would have annoyed me greatly had the lines in this movie not been so hokey and badly delivered. i think i laughed, at most, twice, which is sad since i am easily amused if the lines are halfway witty. if you rent this movie you probably won't die from sheer mortification that such a bad movie could be made, but moonstruck is certainly not a good enough movie to buy!\n",
      "Predicted\t0\n",
      "Actual\t0\n",
      "\n",
      "re's. garbage and i was going to send it back but my daughter lost the next week.waste of money ~ nothing but static!\n",
      "Predicted\t0\n",
      "Actual\t0\n",
      "\n",
      "microfiber camisole by cinema etoile. disappointed. had purchased other items of theirs, and was not thrilled with this fabric: to shiny.\n",
      "Predicted\t1\n",
      "Actual\t1\n",
      "\n",
      "disappointing - no good for warm lunches. very disappointing. at most this will keep food warm for 2-3 hours. i try to use this for my daughter's school lunch and end up having to pre-heat the flask with boiling water first to reduce heat transfer when i put in the extremely hot food, and then also wrap the whole flask in kitchen towels, before putting this in an insulated lunch box. it provides a lukewarm lunch after 4 hours.i would expect much better from thermos. the fact that the wall of the flask is warm on filling is a real giveaway that the vacuum (or, more likely, cheap insulation) is just not effective.pros: good portion size, durable, wide mouth, handy folding spoon.cons: does not keep food warm for more than a very short time.\n",
      "Predicted\t1\n",
      "Actual\t1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t,p,l in zip(text, pred.tolist(), labels.tolist()):\n",
    "    print(t)\n",
    "    print(f\"Predicted\\t{p}\")\n",
    "    print(f\"Actual\\t{l}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omler_ori",
   "language": "python",
   "name": "omler_ori"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
