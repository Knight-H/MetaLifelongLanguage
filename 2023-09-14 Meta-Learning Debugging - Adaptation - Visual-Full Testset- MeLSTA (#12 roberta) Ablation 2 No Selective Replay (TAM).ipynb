{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, gc, os, pickle, csv, time, re\n",
    "\n",
    "import datasets.utils\n",
    "import models.utils\n",
    "from models.cls_oml_ori_v2 import OML\n",
    "from models.base_models_ori import LabelAwareReplayMemory, ReplayMemory\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import higher\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation 2: No TAM (Selective Replay)\n",
    "This ablation changes the TAM to normal ER. \n",
    "\n",
    "No \n",
    "```\n",
    "--task_aware\n",
    "--selective_replay\n",
    "--curriculum_replay (because no task!)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model from /data/model_runs/original_oml/aMEL-allquery-order3/OML-order3-id4-2023-10-02_04-01-55.763189.pt\n",
      "Using memory_path from /data/model_runs/original_oml/aMEL-allquery-order3/OML-order3-id4-2023-10-02_04-01-55.763222_memory.pickle\n"
     ]
    }
   ],
   "source": [
    "dataset_order_mapping = {\n",
    "    1: [2, 0, 3, 1, 4],\n",
    "    2: [3, 4, 0, 1, 2],\n",
    "    3: [2, 4, 1, 3, 0],\n",
    "    4: [0, 2, 1, 4, 3]\n",
    "}\n",
    "n_classes = 33\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "\n",
    "BASE_MODEL_PATH = \"/data/model_runs/original_oml/aMEL-allquery-order3\"\n",
    "find_path = \"id4\"\n",
    "\n",
    "model_name = [x for x in os.listdir(BASE_MODEL_PATH) if  re.search(f\"{find_path}.*\\.pt$\", x)][0]\n",
    "model_path = os.path.join(BASE_MODEL_PATH, model_name)\n",
    "memory_name = [x for x in os.listdir(BASE_MODEL_PATH) if  re.search(f\"{find_path}.*memory\\.pickle$\", x)][0]\n",
    "memory_path = os.path.join(BASE_MODEL_PATH, memory_name)\n",
    "print(f\"Using model from {model_path}\")\n",
    "print(f\"Using memory_path from {memory_path}\")\n",
    "\n",
    "use_db_cache = True\n",
    "cache_dir = 'tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"order\": 1,\n",
    "    \"n_epochs\": 1,\n",
    "    \"lr\": 3e-5,\n",
    "    \"inner_lr\": 0.001*5,\n",
    "    \"meta_lr\": 3e-5,\n",
    "    \"model\": \"roberta\",\n",
    "    \"learner\": \"oml\",\n",
    "    \"mini_batch_size\": 16,\n",
    "    \"updates\": 5*1,\n",
    "    \"write_prob\": 0.1,\n",
    "    \"max_length\": 448,\n",
    "    \"seed\": 42,\n",
    "    \"replay_rate\": 0.01,\n",
    "    \"replay_every\": 8000,\n",
    "    \"task_aware\": True,\n",
    "    \"reverse_support\": True,\n",
    "    \"curriculum_replay\": True,\n",
    "    \"pln\": \"1fc\"\n",
    "}\n",
    "sort_score = False # This will be false for ablation!!\n",
    "updates = args[\"updates\"]\n",
    "mini_batch_size = args[\"mini_batch_size\"]\n",
    "order = args[\"order\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args[\"seed\"])\n",
    "random.seed(args[\"seed\"])\n",
    "np.random.seed(args[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the datasets\n",
      "Finished loading all the datasets\n"
     ]
    }
   ],
   "source": [
    "print('Loading the datasets')\n",
    "test_datasets = []\n",
    "for dataset_id in dataset_order_mapping[order]:\n",
    "    test_dataset_file = os.path.join(cache_dir, f\"{dataset_id}.cache\")\n",
    "    if os.path.exists(test_dataset_file):\n",
    "        with open(test_dataset_file, 'rb') as f:\n",
    "            test_dataset = pickle.load(f)\n",
    "    else:\n",
    "        test_dataset = datasets.utils.get_dataset_test(\"\", dataset_id)\n",
    "        print('Loaded {}'.format(test_dataset.__class__.__name__))\n",
    "        test_dataset = datasets.utils.offset_labels(test_dataset)\n",
    "        pickle.dump(test_dataset, open( test_dataset_file, \"wb\" ), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"Pickle saved at {test_dataset_file}\")\n",
    "    test_datasets.append(test_dataset)\n",
    "print('Finished loading all the datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 13:15:50,598 - transformers.tokenization_utils_base - INFO - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "2023-11-10 13:15:50,600 - transformers.tokenization_utils_base - INFO - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "2023-11-10 13:15:51,897 - transformers.configuration_utils - INFO - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "2023-11-10 13:15:51,900 - transformers.configuration_utils - INFO - Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "2023-11-10 13:15:51,957 - transformers.modeling_utils - INFO - loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "2023-11-10 13:15:56,993 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing RobertaModel.\n",
      "\n",
      "2023-11-10 13:15:57,007 - transformers.modeling_utils - INFO - All the weights of RobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "2023-11-10 13:16:00,984 - OML-Log - INFO - Loaded TransformerRLN as RLN\n",
      "2023-11-10 13:16:00,987 - OML-Log - INFO - Loaded LinearPLN as PLN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OML as learner\n"
     ]
    }
   ],
   "source": [
    "learner = OML(device=device, n_classes=n_classes, **args)\n",
    "print('Using {} as learner'.format(learner.__class__.__name__))\n",
    "learner.load_model(model_path)\n",
    "with open(memory_path, 'rb') as f:\n",
    "#     learner.memory = pickle.load(f)\n",
    "    memory_buffer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up task dict for task-aware\n",
    "memory_buffer.task_dict = {\n",
    "    0: list(range(5, 9)), # AG\n",
    "    1: list(range(0, 5)), # Amazon\n",
    "    2: list(range(0, 5)), # Yelp\n",
    "    3: list(range(9, 23)), # DBPedia\n",
    "    4: list(range(23, 33)), # Yahoo\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataclass_mapper = {\n",
    "    \"AGNewsDataset\": 0,\n",
    "    \"AmazonDataset\": 1,\n",
    "    \"YelpDataset\": 2,\n",
    "    \"DBPediaDataset\": 3,\n",
    "    \"YahooAnswersDataset\": 4\n",
    "}\n",
    "dataclass_mapper[\"AGNewsDataset\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Select specific column index per row\n",
    "https://stackoverflow.com/questions/23435782/numpy-selecting-specific-column-index-per-row-by-using-a-list-of-indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, updates, mini_batch_size, dataname=\"\"):\n",
    "    learner.rln.eval()\n",
    "    learner.pln.train()\n",
    "    \n",
    "    all_losses, all_predictions, all_labels, all_label_conf = [], [], [], []\n",
    "    all_adaptation_time = []\n",
    "    # Get Query set first. and then find supporting support set\n",
    "    for query_idx, (query_text, query_labels) in enumerate(dataloader):\n",
    "        print(f\"Query ID {query_idx}/{len(dataloader)}\")\n",
    "        # The task id to optimize to for support set\n",
    "        # task_idx = get_task_from_label_list(query_labels, memory_buffer.task_dict)\n",
    "        task_idx = dataclass_mapper[dataname]\n",
    "        \n",
    "    \n",
    "        support_set = []\n",
    "        for _ in range(updates):\n",
    "            text, labels = memory_buffer.read_batch(batch_size=mini_batch_size)\n",
    "            support_set.append((text, labels))\n",
    "\n",
    "        with higher.innerloop_ctx(learner.pln, learner.inner_optimizer,\n",
    "                                  copy_initial_weights=False, track_higher_grads=False) as (fpln, diffopt):\n",
    "            \n",
    "            INNER_tic = time.time()\n",
    "            # Inner loop\n",
    "            task_predictions, task_labels = [], []\n",
    "            support_loss = []\n",
    "            for text, labels in support_set:\n",
    "                labels = torch.tensor(labels).to(device)\n",
    "                input_dict = learner.rln.encode_text(text)\n",
    "                _repr = learner.rln(input_dict)\n",
    "                output = fpln(_repr)\n",
    "                loss = learner.loss_fn(output, labels)\n",
    "                diffopt.step(loss)\n",
    "                pred = models.utils.make_prediction(output.detach())\n",
    "                support_loss.append(loss.item())\n",
    "                task_predictions.extend(pred.tolist())\n",
    "                task_labels.extend(labels.tolist())\n",
    "            INNER_toc = time.time() - INNER_tic\n",
    "            all_adaptation_time.append(INNER_toc)\n",
    "\n",
    "            if updates > 0: # You can remove this if, but then it's just ugly errors.\n",
    "                acc, prec, rec, f1 = models.utils.calculate_metrics(task_predictions, task_labels)\n",
    "                print('Support set metrics: Loss = {:.4f}, accuracy = {:.4f}, precision = {:.4f}, '\n",
    "                            'recall = {:.4f}, F1 score = {:.4f}'.format(np.mean(support_loss), acc, prec, rec, f1))\n",
    "\n",
    "            # Query set is now here!\n",
    "            query_labels = torch.tensor(query_labels).to(device)\n",
    "            query_input_dict = learner.rln.encode_text(query_text)\n",
    "            with torch.no_grad():\n",
    "                query_repr = learner.rln(query_input_dict)\n",
    "                query_output = fpln(query_repr) # Output has size of torch.Size([16, 33]) [BATCH, CLASSES]\n",
    "                query_loss = learner.loss_fn(query_output, query_labels)\n",
    "            query_loss = query_loss.item()\n",
    "            # print(output.detach().size())\n",
    "            # output.detach().max(-1) max on each Batch, which will return [0] max, [1] indices\n",
    "            query_output_softmax = F.softmax(query_output, -1)\n",
    "            query_label_conf = query_output_softmax[np.arange(len(query_output_softmax)), query_labels] # Select labels in the softmax of 33 classes\n",
    "\n",
    "            query_pred = models.utils.make_prediction(query_output.detach())\n",
    "            query_acc, query_prec, query_rec, query_f1 = models.utils.calculate_metrics(query_pred.tolist(), query_labels.tolist())\n",
    "            \n",
    "            print('Query set metrics: Loss = {:.4f}, accuracy = {:.4f}, precision = {:.4f}, '\n",
    "                'recall = {:.4f}, F1 score = {:.4f}'.format(np.mean(query_loss), query_acc, query_prec, query_rec, query_f1))\n",
    "\n",
    "            all_losses.append(query_loss)\n",
    "            all_predictions.extend(query_pred.tolist())\n",
    "            all_labels.extend(query_labels.tolist())\n",
    "            all_label_conf.extend(query_label_conf.tolist())\n",
    "\n",
    "    acc, prec, rec, f1 = models.utils.calculate_metrics(all_predictions, all_labels)\n",
    "    print('Test metrics: Loss = {:.4f}, accuracy = {:.4f}, precision = {:.4f}, recall = {:.4f}, '\n",
    "                'F1 score = {:.4f}'.format(np.mean(all_losses), acc, prec, rec, f1))\n",
    "    return acc, prec, rec, f1, all_predictions, all_labels, all_label_conf, all_adaptation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Testing on test set starts here----------\n",
      "Testing on YelpDataset\n",
      "Query ID 0/475\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 22.00 MiB (GPU 1; 31.75 GiB total capacity; 1.80 GiB already allocated; 4.50 MiB free; 1.85 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting on \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(test_dataset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[1;32m     11\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mDataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mmini_batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m                                   collate_fn\u001b[38;5;241m=\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mbatch_encode)\n\u001b[0;32m---> 13\u001b[0m acc, prec, rec, f1, all_pred, all_label, all_label_conf, all_adaptation_time \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmini_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m data_ids \u001b[38;5;241m=\u001b[39m [test_dataset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(all_label))]\n\u001b[1;32m     17\u001b[0m data_for_visual\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(data_ids, all_label, all_label_conf, all_pred)))\n",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataloader, updates, mini_batch_size, dataname)\u001b[0m\n\u001b[1;32m     28\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(labels)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m input_dict \u001b[38;5;241m=\u001b[39m learner\u001b[38;5;241m.\u001b[39mrln\u001b[38;5;241m.\u001b[39mencode_text(text)\n\u001b[0;32m---> 30\u001b[0m _repr \u001b[38;5;241m=\u001b[39m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrln\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m output \u001b[38;5;241m=\u001b[39m fpln(_repr)\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m learner\u001b[38;5;241m.\u001b[39mloss_fn(output, labels)\n",
      "File \u001b[0;32m~/MetaLifelongLanguage/env/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/MetaLifelongLanguage/models/base_models_ori.py:98\u001b[0m, in \u001b[0;36mTransformerRLN.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     _, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], attention_mask\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     _, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/MetaLifelongLanguage/env/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/MetaLifelongLanguage/env/lib/python3.8/site-packages/transformers/modeling_bert.py:755\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    750\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    752\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    753\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids, position_ids\u001b[38;5;241m=\u001b[39mposition_ids, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids, inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds\n\u001b[1;32m    754\u001b[0m )\n\u001b[0;32m--> 755\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    765\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output)\n",
      "File \u001b[0;32m~/MetaLifelongLanguage/env/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/MetaLifelongLanguage/env/lib/python3.8/site-packages/transformers/modeling_bert.py:433\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    424\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    425\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    426\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    430\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 433\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/MetaLifelongLanguage/env/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/MetaLifelongLanguage/env/lib/python3.8/site-packages/transformers/modeling_bert.py:370\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    363\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    369\u001b[0m ):\n\u001b[0;32m--> 370\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    374\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[0;32m~/MetaLifelongLanguage/env/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/MetaLifelongLanguage/env/lib/python3.8/site-packages/transformers/modeling_bert.py:314\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    307\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    313\u001b[0m ):\n\u001b[0;32m--> 314\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    318\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/MetaLifelongLanguage/env/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/MetaLifelongLanguage/env/lib/python3.8/site-packages/transformers/modeling_bert.py:232\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     mixed_key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(hidden_states)\n\u001b[0;32m--> 232\u001b[0m     mixed_value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[1;32m    235\u001b[0m key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_key_layer)\n",
      "File \u001b[0;32m~/MetaLifelongLanguage/env/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/MetaLifelongLanguage/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:94\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MetaLifelongLanguage/env/lib/python3.8/site-packages/torch/nn/functional.py:1753\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight):\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1753\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 1; 31.75 GiB total capacity; 1.80 GiB already allocated; 4.50 MiB free; 1.85 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "print('----------Testing on test set starts here----------')\n",
    "\n",
    "accuracies, precisions, recalls, f1s = [], [], [], []\n",
    "all_adapt_time = []\n",
    "# Data for Visualization: [data_idx, label, label_conf, pred]\n",
    "data_for_visual = []\n",
    "\n",
    "for test_dataset in test_datasets:\n",
    "    print('Testing on {}'.format(test_dataset.__class__.__name__))\n",
    "    test_dataloader = data.DataLoader(test_dataset, batch_size=mini_batch_size, shuffle=False,\n",
    "                                      collate_fn=datasets.utils.batch_encode)\n",
    "    acc, prec, rec, f1, all_pred, all_label, all_label_conf, all_adaptation_time = evaluate(dataloader=test_dataloader, updates=updates, \n",
    "                                                mini_batch_size=mini_batch_size, dataname=test_dataset.__class__.__name__)\n",
    "    \n",
    "    data_ids = [test_dataset.__class__.__name__ + str(i) for i in range(len(all_label))]\n",
    "    data_for_visual.extend(list(zip(data_ids, all_label, all_label_conf, all_pred)))\n",
    "    all_adapt_time.extend(all_adaptation_time)\n",
    "#     print(data_ids)\n",
    "#     print(all_label)\n",
    "#     raise Exception(\"BREAKPOINT\")\n",
    "    \n",
    "    accuracies.append(acc)\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "    f1s.append(f1)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"COPY PASTA - not really but ok\")\n",
    "for row in accuracies:\n",
    "    print(row)\n",
    "print()\n",
    "print('Overall test metrics: Accuracy = {:.4f}, precision = {:.4f}, recall = {:.4f}, '\n",
    "            'F1 score = {:.4f}'.format(np.mean(accuracies), np.mean(precisions), np.mean(recalls), np.mean(f1s)))\n",
    "\n",
    "toc = time.time() - tic\n",
    "print(f\"Total Time used: {toc//60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_path0 = os.path.splitext(model_path)[0]\n",
    "csv_filename = _model_path0 + \"_update\"+ str(updates) +\"_results.csv\" # for selective replay\n",
    "with open(csv_filename, 'w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow([\"data_idx\", \"label\", \"label_conf\", \"pred\"])\n",
    "    csv_writer.writerows(data_for_visual)\n",
    "print(f\"Done writing CSV File at {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Time for Inference\n",
    "_model_path0 = os.path.splitext(model_path)[0]\n",
    "time_txt_filename = _model_path0 + \"_update\"+ str(updates) +\"_time_inference.csv\" \n",
    "with open(time_txt_filename, 'w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow([\"time_id\", \"time\"])\n",
    "    csv_writer.writerow([\"Total Time\", f\"{toc//60} minutes\"])\n",
    "    csv_writer.writerow([\"mean Adapt Time\", f\"{np.mean(all_adapt_time)} s\"])\n",
    "print(f\"Done writing Time CSV File at {time_txt_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metalifelong",
   "language": "python",
   "name": "metalifelong"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
