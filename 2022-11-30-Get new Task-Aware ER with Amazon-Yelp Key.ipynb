{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.text_classification_dataset import MAX_TRAIN_SIZE, MAX_VAL_SIZE\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "import datasets.utils\n",
    "from models.cls_oml_ori_v2 import OML\n",
    "from models.base_models_ori import LabelAwareReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ordering of the datasets\n",
    "dataset_order_mapping = {\n",
    "    1: [2, 0, 3, 1, 4],\n",
    "    2: [3, 4, 0, 1, 2],\n",
    "    3: [2, 4, 1, 3, 0],\n",
    "    4: [0, 2, 1, 4, 3]\n",
    "}\n",
    "n_classes = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"order\": 1,\n",
    "    \"n_epochs\": 1,\n",
    "    \"lr\": 3e-5,\n",
    "    \"inner_lr\": 0.001*10,\n",
    "    \"meta_lr\": 3e-5,\n",
    "    \"model\": \"bert\",\n",
    "    \"learner\": \"oml\",\n",
    "    \"mini_batch_size\": 16,\n",
    "    \"updates\": 5*1,\n",
    "    \"write_prob\": 1.0,\n",
    "    \"max_length\": 448,\n",
    "    \"seed\": 42,\n",
    "    \"replay_rate\": 0.01,\n",
    "    \"replay_every\": 9600\n",
    "}\n",
    "updates = args[\"updates\"]\n",
    "mini_batch_size = args[\"mini_batch_size\"]\n",
    "order = args[\"order\"]\n",
    "seed = args[\"seed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the datasets\n",
      "Finished loading all the datasets\n"
     ]
    }
   ],
   "source": [
    "## Added db caching for ease of load\n",
    "use_db_cache = True\n",
    "cache_dir = '/data/omler_data/tmp'\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load the datasets\n",
    "print('Loading the datasets')\n",
    "train_datasets, val_datasets, test_datasets = [], [], []\n",
    "for dataset_id in dataset_order_mapping[order]:\n",
    "\n",
    "    # KNIGHT EDIT TO MAKE IT LOAD TRAINING DATA FAST!\n",
    "    train_dataset_file = os.path.join(cache_dir, f\"train-{dataset_id}.cache\")\n",
    "    if os.path.exists(train_dataset_file):\n",
    "        with open(train_dataset_file, 'rb') as f:\n",
    "            train_dataset = pickle.load(f)\n",
    "    else:\n",
    "        train_dataset = datasets.utils.get_dataset_train(\"\", dataset_id)\n",
    "        print('Loaded {}'.format(train_dataset.__class__.__name__))\n",
    "        train_dataset = datasets.utils.offset_labels(train_dataset)\n",
    "        pickle.dump(train_dataset, open( train_dataset_file, \"wb\" ), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"Pickle saved at {train_dataset_file}\")\n",
    "\n",
    "    train_dataset, val_dataset = datasets.utils.get_train_val_split(dataset=train_dataset,\n",
    "                                                                    train_size=MAX_TRAIN_SIZE,\n",
    "                                                                    val_size=MAX_VAL_SIZE)\n",
    "    train_datasets.append(train_dataset)\n",
    "    val_datasets.append(val_dataset)\n",
    "print('Finished loading all the datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_task_class_key(task_dict):\n",
    "    task_class_list = []\n",
    "    for task_idx, class_list in task_dict.items():\n",
    "        task_class_list.extend([f\"{task_idx}|{class_idx}\" for class_idx in class_list])\n",
    "    return task_class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0|5', '0|6', '0|7', '0|8', '1|0', '1|1', '1|2', '1|3', '1|4', '2|0', '2|1', '2|2', '2|3', '2|4', '3|9', '3|10', '3|11', '3|12', '3|13', '3|14', '3|15', '3|16', '3|17', '3|18', '3|19', '3|20', '3|21', '3|22', '4|23', '4|24', '4|25', '4|26', '4|27', '4|28', '4|29', '4|30', '4|31', '4|32']\n"
     ]
    }
   ],
   "source": [
    "task_dict = {\n",
    "    0: list(range(5, 9)), # AG\n",
    "    1: list(range(0, 5)), # Amazon\n",
    "    2: list(range(0, 5)), # Yelp\n",
    "    3: list(range(9, 23)), # DBPedia\n",
    "    4: list(range(23, 33)), # Yahoo\n",
    "}\n",
    "task_class_list = convert_to_task_class_key(task_dict)\n",
    "print(task_class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_buffer = LabelAwareReplayMemory(write_prob=1., tuple_size=2, n_classes=33, \\\n",
    "                                     validation_split=0., task_dict=task_dict, task_aware=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with train_idx: 0 on task_idx\n",
      "Terminating training as all the data is seen\n",
      "Starting with train_idx: 1 on task_idx\n",
      "Terminating training as all the data is seen\n",
      "Starting with train_idx: 2 on task_idx\n",
      "Terminating training as all the data is seen\n",
      "Starting with train_idx: 3 on task_idx\n",
      "Terminating training as all the data is seen\n",
      "Starting with train_idx: 4 on task_idx\n",
      "Terminating training as all the data is seen\n"
     ]
    }
   ],
   "source": [
    "episode_id = 0\n",
    "        \n",
    "for train_idx, train_dataset in enumerate(train_datasets):\n",
    "    task_idx = dataset_order_mapping[order][train_idx]\n",
    "    print('Starting with train_idx: {} on task_idx'.format(train_idx, task_idx))\n",
    "    \n",
    "    # Change to each dataset.\n",
    "    train_dataloader = iter(data.DataLoader(data.ConcatDataset([train_dataset]), batch_size=mini_batch_size, shuffle=False,\n",
    "                                            collate_fn=datasets.utils.batch_encode))\n",
    "\n",
    "    while True:\n",
    "        is_break = False\n",
    "        \n",
    "        # Inner loop\n",
    "        support_set = []\n",
    "        task_predictions, task_labels = [], []\n",
    "        for _ in range(updates):\n",
    "            try:\n",
    "                text, labels = next(train_dataloader)\n",
    "                support_set.append((text, labels))\n",
    "            except StopIteration:\n",
    "                is_break = True\n",
    "                print('Terminating training as all the data is seen')\n",
    "                break\n",
    "        \n",
    "        if is_break:\n",
    "            break\n",
    "\n",
    "        for text, labels in support_set:\n",
    "            memory_buffer.write_batch(text, labels, task_id=task_idx)\n",
    "\n",
    "        episode_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ER on path: /data/model_runs/original_oml/aOML-order1-inlr010-2022-08-30-sr-query/OML-order1-id4-2022-08-30_05-21-18.854228_memory_newkey.pickle\n"
     ]
    }
   ],
   "source": [
    "#SAVE MODEL AND MEMORY EVERY EPOCH\n",
    "model_path = \"/data/model_runs/original_oml/aOML-order1-inlr010-2022-08-30-sr-query/OML-order1-id4-2022-08-30_05-21-18.854228.pt\"\n",
    "_model_path0 = os.path.splitext(model_path)[0]\n",
    "MEMORY_SAVE_LOC = _model_path0 + \"_memory_newkey.pickle\"\n",
    "pickle.dump( memory_buffer, open( MEMORY_SAVE_LOC, \"wb\" ), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('Saving ER on path: {}'.format(MEMORY_SAVE_LOC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2|3', '2|2', '2|0', '2|4', '2|1', '0|8', '0|6', '0|5', '0|7', '3|18', '3|19', '3|14', '3|10', '3|16', '3|12', '3|20', '3|11', '3|21', '3|17', '3|15', '3|9', '3|22', '3|13', '1|1', '1|2', '1|3', '1|4', '1|0', '4|25', '4|28', '4|31', '4|32', '4|23', '4|24', '4|30', '4|26', '4|27', '4|29'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_buffer.buffer_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"my daughter and i went there on saturday night. the place wasn't very busy, only one other table taken. we sat down and were immediately greeted by the waitress. she was very helpful, explained the menu to me, and was also very patient as i had never had indian food before. my daughter and i both got a chicken dish, chicken tikki masala and tandoori chicken megan's was a boneless chicken and mine had bones in. both were pretty spicy, which surprised me, didn't really expect that, but what did i know, it was all new to me. they were both very good, and i did enjoy them. we also got rice and naan, which was ok, and she brought us out some sauces, the best being the mint chutney. that one little sauce was my favorite thing out of the whole meal. i would spread it on toast and eat it if i could! all in all, i really enjoyed this place. if we are in the neighborhood i would definitely drop in again, not only for the food, but for the fantastic service. the service, if i could rate it separately, would get a 5plus in my book!\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_buffer.buffer_dict[\"2|3\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omler_ori",
   "language": "python",
   "name": "omler_ori"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
