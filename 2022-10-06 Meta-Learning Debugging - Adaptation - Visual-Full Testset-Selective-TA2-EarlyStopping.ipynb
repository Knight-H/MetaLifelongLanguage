{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, gc, os, pickle, csv, time, json \n",
    "\n",
    "import datasets.utils\n",
    "import models.utils\n",
    "from models.cls_oml_ori_v2 import OML\n",
    "from models.base_models_ori import LabelAwareReplayMemory\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import higher\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_order_mapping = {\n",
    "    1: [2, 0, 3, 1, 4],\n",
    "    2: [3, 4, 0, 1, 2],\n",
    "    3: [2, 4, 1, 3, 0],\n",
    "    4: [0, 2, 1, 4, 3]\n",
    "}\n",
    "n_classes = 33\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "# model_path = \"/data/model_runs/original_oml/aOML-order1-2022-07-18/OML-order1-id4-2022-07-18_17-53-13.518612.pt\"\n",
    "# model_path = \"/data/model_runs/original_oml/aOML-order1-inlr002-2022-07-31/OML-order1-id4-2022-07-31_14-53-46.456804.pt\"\n",
    "# model_path = \"/data/model_runs/original_oml/aOML-order1-inlr005-2022-07-31/OML-order1-id4-2022-07-31_18-47-41.477968.pt\"\n",
    "# model_path = \"/data/model_runs/original_oml/aOML-order1-inlr005-up20-2022-08-01/OML-order1-id4-2022-08-01_14-45-55.869765.pt\"\n",
    "# model_path = \"/data/model_runs/original_oml/aOML-order1-inlr010-2022-07-31/OML-order1-id4-2022-07-31_21-18-36.241546.pt\"\n",
    "# model_path = \"/data/model_runs/original_oml/aOML-order1-inlr020-2022-08-16/OML-order1-id4-2022-08-16_11-37-19.424113.pt\"\n",
    "# model_path = \"/data/model_runs/original_oml/aOML-order1-inlr050-2022-08-16/OML-order1-id4-2022-08-16_14-16-12.167637.pt\"\n",
    "\n",
    "# v. SR \n",
    "# model_path = \"/data/model_runs/original_oml/aOML-order1-inlr010-2022-08-29-sr/OML-order1-id4-2022-08-29_18-10-31.695669.pt\"\n",
    "# v. SR Query\n",
    "model_path = \"/data/model_runs/original_oml/aOML-order1-inlr010-2022-08-30-sr-query/OML-order1-id4-2022-08-30_05-21-18.854228.pt\"\n",
    "\n",
    "# memory_path = \"/data/model_runs/original_oml/aOML-order1-2022-07-18/OML-order1-id4-2022-07-18_17-53-13.518639_memory.pickle\"\n",
    "# memory_path = \"/data/model_runs/original_oml/aOML-order1-inlr002-2022-07-31/OML-order1-id4-2022-07-31_14-53-46.456828_memory.pickle\"\n",
    "# memory_path = \"/data/model_runs/original_oml/aOML-order1-inlr005-2022-07-31/OML-order1-id4-2022-07-31_18-47-41.477992_memory.pickle\"\n",
    "# memory_path = \"/data/model_runs/original_oml/aOML-order1-inlr005-up20-2022-08-01/OML-order1-id4-2022-08-01_14-45-55.869797_memory.pickle\"\n",
    "# memory_path = \"/data/model_runs/original_oml/aOML-order1-inlr010-2022-07-31/OML-order1-id4-2022-07-31_21-18-36.241572_memory.pickle\"\n",
    "# memory_path = \"/data/model_runs/original_oml/aOML-order1-inlr020-2022-08-16/OML-order1-id4-2022-08-16_11-37-19.424139_memory.pickle\"\n",
    "# memory_path = \"/data/model_runs/original_oml/aOML-order1-inlr050-2022-08-16/OML-order1-id4-2022-08-16_14-16-12.167666_memory.pickle\"\n",
    "# v. SR \n",
    "# memory_path = \"/data/model_runs/original_oml/aOML-order1-inlr010-2022-08-29-sr/OML-order1-id4-2022-08-29_18-10-31.695692_memory.pickle\"\n",
    "# v. SR Query\n",
    "memory_path = \"/data/model_runs/original_oml/aOML-order1-inlr010-2022-08-30-sr-query/OML-order1-id4-2022-08-30_05-21-18.854254_memory.pickle\"\n",
    "\n",
    "\n",
    "# new_memory_path, ext = os.path.splitext(memory_path)\n",
    "# new_memory_path = new_memory_path + \"_label\" + ext\n",
    "\n",
    "use_db_cache = True\n",
    "cache_dir = 'tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"order\": 1,\n",
    "    \"n_epochs\": 1,\n",
    "    \"lr\": 3e-5,\n",
    "    \"inner_lr\": 0.001*10,\n",
    "    \"meta_lr\": 3e-5,\n",
    "    \"model\": \"bert\",\n",
    "    \"learner\": \"oml\",\n",
    "    \"mini_batch_size\": 16,\n",
    "    \"updates\": 5*10,\n",
    "    \"write_prob\": 1.0,\n",
    "    \"max_length\": 448,\n",
    "    \"seed\": 42,\n",
    "    \"replay_rate\": 0.01,\n",
    "    \"replay_every\": 9600\n",
    "}\n",
    "updates = args[\"updates\"]\n",
    "mini_batch_size = args[\"mini_batch_size\"]\n",
    "order = args[\"order\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args[\"seed\"])\n",
    "random.seed(args[\"seed\"])\n",
    "np.random.seed(args[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the datasets\n",
      "Finished loading all the datasets\n"
     ]
    }
   ],
   "source": [
    "print('Loading the datasets')\n",
    "test_datasets = []\n",
    "for dataset_id in dataset_order_mapping[order]:\n",
    "    test_dataset_file = os.path.join(cache_dir, f\"{dataset_id}.cache\")\n",
    "    if os.path.exists(test_dataset_file):\n",
    "        with open(test_dataset_file, 'rb') as f:\n",
    "            test_dataset = pickle.load(f)\n",
    "    else:\n",
    "        test_dataset = datasets.utils.get_dataset_test(\"\", dataset_id)\n",
    "        print('Loaded {}'.format(test_dataset.__class__.__name__))\n",
    "        test_dataset = datasets.utils.offset_labels(test_dataset)\n",
    "        pickle.dump(test_dataset, open( test_dataset_file, \"wb\" ), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"Pickle saved at {test_dataset_file}\")\n",
    "    test_datasets.append(test_dataset)\n",
    "print('Finished loading all the datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [2], 1: [3]}\n",
      "{0: [3, 1], 1: [2, 4]}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "buffer_dict = {\n",
    "    0: [1,2,3],\n",
    "    1: [2,3,4]\n",
    "}\n",
    "\n",
    "\n",
    "buffer_dict_valid = {} # Reset the buffer_dict_valid\n",
    "for class_idx, buffer_list in buffer_dict.items():\n",
    "    buffer_dict_valid[class_idx] = []\n",
    "    for i in range(2):\n",
    "        pop_data = buffer_dict[class_idx].pop(random.randint(0,len(buffer_dict[class_idx])-1))\n",
    "        buffer_dict_valid[class_idx].append(pop_data)\n",
    "print(buffer_dict)\n",
    "print(buffer_dict_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-08 17:52:43,038 - transformers.tokenization_utils_base - INFO - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-10-08 17:52:44,149 - transformers.configuration_utils - INFO - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "2022-10-08 17:52:44,154 - transformers.configuration_utils - INFO - Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2022-10-08 17:52:46,584 - transformers.modeling_utils - INFO - loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "2022-10-08 17:52:49,368 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "2022-10-08 17:52:49,370 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "2022-10-08 17:52:53,866 - OML-Log - INFO - Loaded TransformerRLN as RLN\n",
      "2022-10-08 17:52:53,867 - OML-Log - INFO - Loaded LinearPLN as PLN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OML as learner\n"
     ]
    }
   ],
   "source": [
    "learner = OML(device=device, n_classes=n_classes, **args)\n",
    "print('Using {} as learner'.format(learner.__class__.__name__))\n",
    "learner.load_model(model_path)\n",
    "with open(memory_path, 'rb') as f:\n",
    "#     learner.memory = pickle.load(f)\n",
    "    memory_buffer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up task dict for task-aware\n",
    "memory_buffer.task_dict = {\n",
    "    0: list(range(5, 9)), # AG\n",
    "    1: list(range(0, 5)), # Amazon\n",
    "    2: list(range(0, 5)), # Yelp\n",
    "    3: list(range(9, 23)), # DBPedia\n",
    "    4: list(range(23, 33)), # Yahoo\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataclass_mapper = {\n",
    "    \"AGNewsDataset\": 0,\n",
    "    \"AmazonDataset\": 1,\n",
    "    \"YelpDataset\": 2,\n",
    "    \"DBPediaDataset\": 3,\n",
    "    \"YahooAnswersDataset\": 4\n",
    "}\n",
    "dataclass_mapper[\"AGNewsDataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For hacking the early stopping to work!!\n",
    "# Split the buffer_dict (train) to buffer_dict_valid (validation)\n",
    "memory_buffer.split_train_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45559"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(memory_buffer.buffer_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(memory_buffer.buffer_dict_valid[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Select specific column index per row\n",
    "https://stackoverflow.com/questions/23435782/numpy-selecting-specific-column-index-per-row-by-using-a-list-of-indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns loss,preds,labels, labels_conf\n",
    "def validate(fpln, validation_set, batch_size = 16):\n",
    "    all_valid_losses, all_valid_preds, all_valid_labels, all_valid_label_conf = [], [], [], []\n",
    "    \n",
    "    # https://stackoverflow.com/questions/2231663/slicing-a-list-into-a-list-of-sub-lists\n",
    "    for i in range(0, len(validation_set[1]), batch_size):\n",
    "        valid_text = validation_set[0][i:i+batch_size]\n",
    "        valid_labels = validation_set[1][i:i+batch_size]\n",
    "        \n",
    "        valid_labels = torch.tensor(valid_labels).to(device)\n",
    "        valid_input_dict = learner.rln.encode_text(valid_text)\n",
    "        valid_repr = learner.rln(valid_input_dict)\n",
    "        valid_output = fpln(valid_repr) # Output has size of torch.Size([16, 33]) [BATCH, CLASSES]\n",
    "        valid_loss = learner.loss_fn(valid_output, valid_labels)\n",
    "        valid_loss = valid_loss.item()\n",
    "\n",
    "        # output.detach().max(-1) max on each Batch, which will return [0] max, [1] indices\n",
    "        valid_output_softmax = F.softmax(valid_output, -1)\n",
    "        valid_label_conf = valid_output_softmax[np.arange(len(valid_output_softmax)), valid_labels] # Select labels in the softmax of 33 classes\n",
    "\n",
    "        valid_pred = models.utils.make_prediction(valid_output.detach())\n",
    "        \n",
    "        \n",
    "        # Put in things to return\n",
    "        # all_valid_losses.extend(valid_loss)\n",
    "        all_valid_preds.extend(valid_pred.tolist())\n",
    "        all_valid_labels.extend(valid_labels.tolist())\n",
    "        all_valid_label_conf.extend(valid_label_conf.tolist())\n",
    "    return all_valid_preds, all_valid_labels, all_valid_label_conf # removed loss, since no need\n",
    "    \n",
    "# Compare diff results between the unadapted vs adapted\n",
    "# Returns Dictionary of class_idx -> [ a - n, ...  ] for each i (300). Can np.sum() or np.mean() later\n",
    "# validate_labels = The labels (Shared)\n",
    "# validate_label_conf_0 = The label conf of validate_0\n",
    "# validate_label_conf_n = The label conf of validate_n\n",
    "def calculate_diff_class(validate_labels, validate_label_conf_0, validate_label_conf_n, initial_dict={}): \n",
    "    # Adapted confs - NonAdapted Confs (a-n)\n",
    "    validate_label_conf_diff = np.array(validate_label_conf_n) - np.array(validate_label_conf_0)\n",
    "    \n",
    "    # The dictionary to return  class_idx -> [ a - n, ...  ] \n",
    "    return_dict = initial_dict.copy()\n",
    "    for i, class_idx in enumerate(validate_labels):\n",
    "        # Filter conf_diff by class\n",
    "        return_dict[class_idx] = return_dict.get(class_idx, []) + [validate_label_conf_diff[i]]\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, updates, mini_batch_size, dataname=\"\"):\n",
    "    learner.rln.eval()\n",
    "    learner.pln.train()\n",
    "    \n",
    "    all_losses, all_predictions, all_labels, all_label_conf = [], [], [], []\n",
    "    \n",
    "    # Map of update# --> array<item>\n",
    "    # THIS IS NOT CORRECT ! WILL GET ONLY 1 QUERY IDX!\n",
    "    valid_losses, valid_preds, valid_labels, valid_label_conf = {}, {}, {}, {}\n",
    "    valid_class_diff = {}\n",
    "    \n",
    "    all_adaptation_time = []\n",
    "    # Get Query set first. and then find supporting support set\n",
    "    for query_idx, (query_text, query_labels) in enumerate(dataloader):\n",
    "        \n",
    "        # Only get the first 20 queries! (as sample to plot)\n",
    "        if (query_idx > 2):\n",
    "            continue\n",
    "        \n",
    "        print(f\"Query ID {query_idx}/{len(dataloader)}\")\n",
    "        # The task id to optimize to for support set\n",
    "        # task_idx = get_task_from_label_list(query_labels, memory_buffer.task_dict)\n",
    "        task_idx = dataclass_mapper[dataname]\n",
    "        \n",
    "    \n",
    "        support_set = []\n",
    "        for _ in range(updates):\n",
    "            text, labels = memory_buffer.read_batch_task(batch_size=mini_batch_size, task_idx=task_idx)\n",
    "            support_set.append((text, labels))\n",
    "        \n",
    "\n",
    "        # Get Validation Set of the task_idx - nah\n",
    "        # Get Validation Set of the whole set\n",
    "        validation_set = memory_buffer.read_batch_validation()\n",
    "        \n",
    "        with higher.innerloop_ctx(learner.pln, learner.inner_optimizer,\n",
    "                                  copy_initial_weights=False, track_higher_grads=False) as (fpln, diffopt):\n",
    "            \n",
    "            # Test validation_set BEFORE the update (update=0)\n",
    "            with torch.no_grad():\n",
    "                #validation_results_0 = validate(fpln, validation_set, batch_size=mini_batch_size)\n",
    "                all_valid_preds_0, all_valid_labels_0, all_valid_label_conf_0 \\\n",
    "                        = validate(fpln, validation_set, batch_size=mini_batch_size)\n",
    "                #calculate_diff_class() no need to calculate diff since it's 0\n",
    "                #valid_losses[0] = all_valid_losses_0\n",
    "                valid_preds[0] = valid_preds.get(0, []) + all_valid_preds_0\n",
    "                valid_labels[0] = valid_labels.get(0, []) + all_valid_labels_0\n",
    "                valid_label_conf[0] = valid_label_conf.get(0, []) + all_valid_label_conf_0\n",
    "                \n",
    "            \n",
    "            INNER_tic = time.time()\n",
    "            # Inner loop\n",
    "            task_predictions, task_labels = [], []\n",
    "            support_loss = []\n",
    "            for adapt_idx, (text, labels) in enumerate(support_set):\n",
    "                \n",
    "                labels = torch.tensor(labels).to(device)\n",
    "                input_dict = learner.rln.encode_text(text)\n",
    "                _repr = learner.rln(input_dict)\n",
    "                output = fpln(_repr)\n",
    "                loss = learner.loss_fn(output, labels)\n",
    "                diffopt.step(loss)\n",
    "                pred = models.utils.make_prediction(output.detach())\n",
    "                support_loss.append(loss.item())\n",
    "                task_predictions.extend(pred.tolist())\n",
    "                task_labels.extend(labels.tolist())\n",
    "                \n",
    "                \n",
    "                # Test validation_set DURING the update (update=i)\n",
    "                # Every 5\n",
    "                #print(\"ADAPT IDX \", adapt_idx)\n",
    "                #print(text)\n",
    "                #print(pred.tolist())\n",
    "                if((adapt_idx + 1) % 5 == 0):\n",
    "                #if(True):\n",
    "                    with torch.no_grad():\n",
    "                        all_valid_preds_n, all_valid_labels_n, all_valid_label_conf_n = \\\n",
    "                                validate(fpln, validation_set, batch_size=mini_batch_size)\n",
    "                        class_diff_dict = calculate_diff_class(all_valid_labels_0, all_valid_label_conf_0, all_valid_label_conf_n, valid_class_diff[adapt_idx+1])\n",
    "                        #valid_losses[adapt_idx+1] = all_valid_losses_n\n",
    "                        valid_preds[adapt_idx+1] = valid_preds.get(adapt_idx+1, []) +  all_valid_preds_n\n",
    "                        valid_labels[adapt_idx+1] = valid_labels.get(adapt_idx+1, []) + all_valid_labels_n\n",
    "                        valid_label_conf[adapt_idx+1] = valid_label_conf.get(adapt_idx+1, []) + all_valid_label_conf_n\n",
    "                        valid_class_diff[adapt_idx+1] = class_diff_dict\n",
    "                \n",
    "                \n",
    "            INNER_toc = time.time() - INNER_tic\n",
    "            all_adaptation_time.append(INNER_toc)\n",
    "            print(\"Adapt Time: \"+ str(INNER_toc//60) +\" minutes\" )\n",
    "\n",
    "            acc, prec, rec, f1 = models.utils.calculate_metrics(task_predictions, task_labels)\n",
    "\n",
    "            print('Support set metrics: Loss = {:.4f}, accuracy = {:.4f}, precision = {:.4f}, '\n",
    "                        'recall = {:.4f}, F1 score = {:.4f}'.format(np.mean(support_loss), acc, prec, rec, f1))\n",
    "\n",
    "            # Query set is now here!\n",
    "            query_labels = torch.tensor(query_labels).to(device)\n",
    "            query_input_dict = learner.rln.encode_text(query_text)\n",
    "            with torch.no_grad():\n",
    "                query_repr = learner.rln(query_input_dict)\n",
    "                query_output = fpln(query_repr) # Output has size of torch.Size([16, 33]) [BATCH, CLASSES]\n",
    "                query_loss = learner.loss_fn(query_output, query_labels)\n",
    "            query_loss = query_loss.item()\n",
    "            # print(output.detach().size())\n",
    "            # output.detach().max(-1) max on each Batch, which will return [0] max, [1] indices\n",
    "            query_output_softmax = F.softmax(query_output, -1)\n",
    "            query_label_conf = query_output_softmax[np.arange(len(query_output_softmax)), query_labels] # Select labels in the softmax of 33 classes\n",
    "\n",
    "            query_pred = models.utils.make_prediction(query_output.detach())\n",
    "            query_acc, query_prec, query_rec, query_f1 = models.utils.calculate_metrics(query_pred.tolist(), query_labels.tolist())\n",
    "            \n",
    "            print('Query set metrics: Loss = {:.4f}, accuracy = {:.4f}, precision = {:.4f}, '\n",
    "                'recall = {:.4f}, F1 score = {:.4f}'.format(np.mean(query_loss), query_acc, query_prec, query_rec, query_f1))\n",
    "\n",
    "            all_losses.append(query_loss)\n",
    "            all_predictions.extend(query_pred.tolist())\n",
    "            all_labels.extend(query_labels.tolist())\n",
    "            all_label_conf.extend(query_label_conf.tolist())\n",
    "\n",
    "    acc, prec, rec, f1 = models.utils.calculate_metrics(all_predictions, all_labels)\n",
    "    print('Test metrics: Loss = {:.4f}, accuracy = {:.4f}, precision = {:.4f}, recall = {:.4f}, '\n",
    "                'F1 score = {:.4f}'.format(np.mean(all_losses), acc, prec, rec, f1))\n",
    "    return acc, prec, rec, f1, all_predictions, all_labels, all_label_conf, all_adaptation_time, \\\n",
    "             valid_preds, valid_labels, valid_label_conf, valid_class_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4+1)%5==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Testing on test set starts here----------\n",
      "Testing on YelpDataset\n",
      "Query ID 0/475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-08 17:53:41,776 - root - ERROR - Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "2022-10-08 17:53:41,786 - root - INFO - \n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/MetaLifelongLanguage/env_ori/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_5745/4095525302.py\", line 19, in <module>\n",
      "    dataname=test_dataset.__class__.__name__)\n",
      "  File \"/tmp/ipykernel_5745/1448632187.py\", line 43, in evaluate\n",
      "    = validate(fpln, validation_set, batch_size=mini_batch_size)\n",
      "  File \"/tmp/ipykernel_5745/3916900506.py\", line 15, in validate\n",
      "    valid_loss = valid_loss.item()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/MetaLifelongLanguage/env_ori/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/MetaLifelongLanguage/env_ori/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/root/MetaLifelongLanguage/env_ori/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/root/MetaLifelongLanguage/env_ori/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/root/miniconda3/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/root/miniconda3/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/root/miniconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/root/miniconda3/lib/python3.7/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/root/miniconda3/lib/python3.7/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/root/miniconda3/lib/python3.7/inspect.py\", line 685, in getsourcefile\n",
      "    all_bytecode_suffixes = importlib.machinery.DEBUG_BYTECODE_SUFFIXES[:]\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5745/4095525302.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         = evaluate(dataloader=test_dataloader, updates=updates, mini_batch_size=mini_batch_size, \n\u001b[0;32m---> 19\u001b[0;31m                     dataname=test_dataset.__class__.__name__)\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5745/1448632187.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(dataloader, updates, mini_batch_size, dataname)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mall_valid_preds_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_valid_labels_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_valid_label_conf_0\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                         \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0;31m#calculate_diff_class() no need to calculate diff since it's 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5745/3916900506.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(fpln, validation_set, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/MetaLifelongLanguage/env_ori/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/MetaLifelongLanguage/env_ori/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2101\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2102\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2104\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MetaLifelongLanguage/env_ori/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1368\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MetaLifelongLanguage/env_ori/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1268\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             )\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MetaLifelongLanguage/env_ori/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1125\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MetaLifelongLanguage/env_ori/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MetaLifelongLanguage/env_ori/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "print('----------Testing on test set starts here----------')\n",
    "\n",
    "accuracies, precisions, recalls, f1s = [], [], [], []\n",
    "all_adapt_time = []\n",
    "# Data for Visualization: [data_idx, label, label_conf, pred]\n",
    "data_for_visual = []\n",
    "# Data for Validation: [data_idx, update, label, label_conf, pred]\n",
    "data_for_valid = []\n",
    "#all_class_diff = {} # combine all the class diff\n",
    "\n",
    "for test_dataset in test_datasets:\n",
    "    print('Testing on {}'.format(test_dataset.__class__.__name__))\n",
    "    test_dataloader = data.DataLoader(test_dataset, batch_size=mini_batch_size, shuffle=False,\n",
    "                                      collate_fn=datasets.utils.batch_encode)\n",
    "    acc, prec, rec, f1, all_pred, all_label, all_label_conf, all_adaptation_time, \\\n",
    "        valid_preds, valid_labels, valid_label_conf, valid_class_diff \\\n",
    "        = evaluate(dataloader=test_dataloader, updates=updates, mini_batch_size=mini_batch_size, \n",
    "                    dataname=test_dataset.__class__.__name__)\n",
    "    \n",
    "    data_ids = [test_dataset.__class__.__name__ + str(i) for i in range(len(all_label))]\n",
    "    data_for_visual.extend(list(zip(data_ids, all_label, all_label_conf, all_pred)))\n",
    "    all_adapt_time.extend(all_adaptation_time)\n",
    "\n",
    "    data_valid_ids = [test_dataset.__class__.__name__ + str(i) for i in range(len(valid_labels[0]))]\n",
    "    \n",
    "    for adapt_i in valid_labels.keys():\n",
    "        data_for_valid.extend(list(zip(data_valid_ids, [adapt_i]*len(valid_labels[0]), valid_labels[adapt_i], valid_label_conf[adapt_i], valid_preds[adapt_i])))\n",
    "    \n",
    "    print(len(data_valid_ids))\n",
    "    print(len(valid_labels[0]))\n",
    "    print(len(valid_preds[0]))\n",
    "    print(len(valid_label_conf[0]))\n",
    "    \n",
    "    print(valid_class_diff.keys())\n",
    "    print(valid_class_diff[5].keys())\n",
    "    \n",
    "    # Valid Label Conf Pickle Dump\n",
    "    _model_path0 = os.path.splitext(model_path)[0]\n",
    "    valid_label_conf_filename = _model_path0 + \"_update\"+ str(updates) +\"_valid_label_conf_sr_ta3_\" + test_dataset.__class__.__name__ + \".pickle\"\n",
    "    with open(valid_label_conf_filename, \"wb\") as outfile:\n",
    "        pickle.dump(valid_label_conf, outfile)\n",
    "    print(f\"Done writing Pickle File at {valid_label_conf_filename}\")\n",
    "    \n",
    "    # Valid Class Diff Pickle dump\n",
    "    _model_path0 = os.path.splitext(model_path)[0]\n",
    "    valid_class_diff_filename = _model_path0 + \"_update\"+ str(updates) +\"_valid_class_diff_sr_ta3_\" + test_dataset.__class__.__name__ + \".pickle\"\n",
    "    with open(valid_class_diff_filename, \"wb\") as outfile:\n",
    "        pickle.dump(valid_class_diff, outfile)\n",
    "    print(f\"Done writing Pickle File at {valid_class_diff_filename}\")\n",
    "    \n",
    "    \n",
    "    # Combine all_class_diff\n",
    "    #for update_n, class_diff in valid_class_diff.items():\n",
    "    #    all_class_diff[update_n] = all_class_diff.get(update_n, {})\n",
    "    #    for class_idx, data_list in class_diff.items():\n",
    "    #        all_class_diff[update_n][class_idx] = all_class_diff[update_n].get(class_idx, []) + data_list\n",
    "    \n",
    "    accuracies.append(acc)\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "    f1s.append(f1)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"COPY PASTA - not really but ok\")\n",
    "for row in accuracies:\n",
    "    print(row)\n",
    "print()\n",
    "print('Overall test metrics: Accuracy = {:.4f}, precision = {:.4f}, recall = {:.4f}, '\n",
    "            'F1 score = {:.4f}'.format(np.mean(accuracies), np.mean(precisions), np.mean(recalls), np.mean(f1s)))\n",
    "\n",
    "toc = time.time() - tic\n",
    "print(f\"Total Time used: {toc//60} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats\n",
    "```\n",
    "Done writing Pickle File at /data/model_runs/original_oml/aOML-order1-inlr010-2022-08-30-sr-query/OML-order1-id4-2022-08-30_05-21-18.854228_update10_valid_results_sr_ta_YelpDataset.pickle\n",
    "Testing on AGNewsDataset\n",
    "Query ID 0/475\n",
    "Adapt Time: 18.0 minutes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _model_path0 = os.path.splitext(model_path)[0]\n",
    "# csv_filename = _model_path0 + \"_update\"+ str(updates) +\"_results_sr_ta.csv\" # for selective replay\n",
    "# with open(csv_filename, 'w') as csv_file:\n",
    "#     csv_writer = csv.writer(csv_file)\n",
    "#     csv_writer.writerow([\"data_idx\", \"label\", \"label_conf\", \"pred\"])\n",
    "#     csv_writer.writerows(data_for_visual)\n",
    "# print(f\"Done writing CSV File at {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Time for Inference\n",
    "_model_path0 = os.path.splitext(model_path)[0]\n",
    "time_txt_filename = _model_path0 + \"_update\"+ str(updates) +\"_time_inference_sr_ta3_es.csv\" \n",
    "with open(time_txt_filename, 'w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow([\"time_id\", \"time\"])\n",
    "    csv_writer.writerow([\"Total Time\", f\"{toc//60} minutes\"])\n",
    "    csv_writer.writerow([\"mean Adapt Time\", f\"{np.mean(all_adapt_time)} s\"])\n",
    "print(f\"Done writing Time CSV File at {time_txt_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Valid Set\n",
    "_model_path0 = os.path.splitext(model_path)[0]\n",
    "valid_csv_filename = _model_path0 + \"_update\"+ str(updates) +\"_valid_results_sr_ta3_es.csv\" # for selective replay\n",
    "with open(valid_csv_filename, 'w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    # Data for Validation: [data_idx,label, label_conf, pred]\n",
    "    csv_writer.writerow([\"data_idx\",\"adapt_step\", \"label\", \"label_conf\", \"pred\"])\n",
    "    csv_writer.writerows(data_for_valid)\n",
    "print(f\"Done writing CSV File at {valid_csv_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omler_ori",
   "language": "python",
   "name": "omler_ori"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
